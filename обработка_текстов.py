# -*- coding: utf-8 -*-
"""Обработка текстов.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XOrdeZnnEN1LYNXtXl-BmHBxQnuypbmQ
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import files 
import numpy as np  
import pandas as pd 
import matplotlib.pyplot as plt 
import os 
# %matplotlib inline
from tensorflow.keras import utils 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D, BatchNormalization, Embedding, Flatten, Activation 
from tensorflow.keras.preprocessing.text import Tokenizer 
from tensorflow.keras.preprocessing.sequence import pad_sequences 
from sklearn.preprocessing import LabelEncoder 
from sklearn.model_selection import train_test_split 
from google.colab import drive 
import time

drive.mount('/content/drive')
!unzip -q '/content/drive/My Drive/Базы/Тексты писателей.zip' -d /content/texts
!ls "/content/drive/MyDrive/Базы/"
def readText(fileName): 
  f = open(fileName, 'r')      
  text = f.read()           
  text = text.replace("\n", " ") 
  return text                 
className = ["Булгаков", "Макс Фрай", "Стругацкие", "Клиффорд_Саймак", "О.Генри", "Рэй Брэдберри"] 
nClasses = len(className) 
trainText = []
testText = [] 
for i in className:
  for j in os.listdir('texts/'):
    if i in j: 
      if 'Обучающая' in j:  
        trainText.append(readText('texts/' + j)) 
        print(j, 'добавлен в обучающую выборку')
      if 'Тестовая' in j: 
        testText.append(readText('texts/' + j)) 
        print(j, 'добавлен в тестовую выборку') 
  print()
print(len(trainText)) 
print(len(trainText[0]))

cur_time = time.time()
maxWordsCount = 20000
tokenizer = Tokenizer(num_words=maxWordsCount, filters='!"#$%&()*+,-–—./…:;<=>?@[\\]^_`{|}~«»\t\n\xa0\ufeff', lower=True, split=' ', oov_token='unknown', char_level=False)
tokenizer.fit_on_texts(trainText) 
items = list(tokenizer.word_index.items())
print('Время обработки: ', round(time.time() - cur_time, 2), 'c', sep='')
print(items[-10:]) 
print("Размер словаря", len(items))

print("Интересующее слово имеет индекс: ", tokenizer.word_index[input("Уточните слово: ")])

trainWordIndexes = tokenizer.texts_to_sequences(trainText)
testWordIndexes = tokenizer.texts_to_sequences(testText)
print("Взглянем на фрагмент обучающего текста:")
print("В виде оригинального текста:              ", trainText[1][:87])
print("Он же в виде последовательности индексов: ", trainWordIndexes[1][:20], '\n')

tokenizer.texts_to_sequences(['есть'])

print("Статистика по обучающим текстам:")
symbolsTrainText = 0 
wordsTrainText = 0 
for i in range(nClasses): 
  print(className[i], " "*(10-len(className[i])), len(trainText[i]), "символов, ", len(trainWordIndexes[i]), "слов")
  symbolsTrainText += len(trainText[i]) 
  wordsTrainText += len(trainWordIndexes[i])
print('----')
print("В сумме ", symbolsTrainText, " символов, ", wordsTrainText, " слов \n")
print()
print("Статистика по тестовым текстам:")
symbolsTestText = 0 
wordsTestText = 0 
for i in range(nClasses): 
  print(className[i], ' '*(10-len(className[i])), len(testText[i]), "символов, ", len(testWordIndexes[i]), "слов")
  symbolsTestText += len(testText[i]) 
  wordsTestText += len(testWordIndexes[i])
print('----')
print("В сумме ", symbolsTestText, " символов, ", wordsTestText, " слов")

def getSetFromIndexes(wordIndexes, xLen, step):
  xSample = [] 
  wordsLen = len(wordIndexes) 
  index = 0 
  while (index + xLen <= wordsLen):# Идём по всей длине вектора индексов
    xSample.append(wordIndexes[index:index+xLen]) # "Откусываем" векторы длины xLen
    index += step # Смещаеммся вперёд на step
  return xSample

def createSetsMultiClasses(wordIndexes, xLen, step):
  nClasses = len(wordIndexes) 
  classesXSamples = []       
  for wI in wordIndexes:   
    classesXSamples.append(getSetFromIndexes(wI, xLen, step)) 
  xSamples = [] 
  ySamples = [] 
  for t in range(nClasses): 
    xT = classesXSamples[t] 
    for i in range(len(xT)):
      xSamples.append(xT[i]) 
      ySamples.append(utils.to_categorical(t, nClasses)) 
  xSamples = np.array(xSamples) 
  ySamples = np.array(ySamples) 
  return (xSamples, ySamples)

xLen = 1000 
step = 100

cur_time = time.time() 
xTrain, yTrain = createSetsMultiClasses(trainWordIndexes, xLen, step) 
xTest, yTest = createSetsMultiClasses(testWordIndexes, xLen, step) 
print(xTrain.shape)
print(yTrain.shape)
print(xTest.shape)
print(yTest.shape)
print('Время обработки: ', round(time.time() - cur_time, 2), 'c', sep='')

cur_time = time.time() 
xTrain01 = tokenizer.sequences_to_matrix(xTrain.tolist()) 
xTest01 = tokenizer.sequences_to_matrix(xTest.tolist()) 
print(xTrain01.shape)      
print(xTrain01[0][0:100]) 
print('Время обработки: ', round(time.time() - cur_time, 2), 'c', sep='')

model01 = Sequential()
model01.add(Dense(200, input_dim=maxWordsCount, activation="relu"))
model01.add(Dropout(0.25))
model01.add(BatchNormalization())
model01.add(Dense(6, activation='softmax'))
model01.compile(optimizer='adam', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])
history = model01.fit(xTrain01, 
                      yTrain, 
                      epochs=10,
                      batch_size=128,
                      validation_data=(xTest01, yTest))
plt.plot(history.history['accuracy'], 
         label='Доля верных ответов на обучающем наборе')
plt.plot(history.history['val_accuracy'], 
         label='Доля верных ответов на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()